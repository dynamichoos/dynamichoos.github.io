---
title:  "[AI] AI 관련 기본 개념들"
date: "2019-05-05 01:36:28 -0400
categories: AI
---

### AI 관련된 기본적인 정보들을 나눠서 요약하겠습니다.
초반에는 우선은 내용을 하나의 md로 관리하고, 향후에 category를 확장하며 분류할 예정입니다.
각각의 자료 소스는 각각의 항목에 기록 예정입니다.

개념에 해당하는 내용을 적다보니, 조금은 자의적인 해석이나 과장된 내용이 있을 수 있음을 양해 부탁드립니다.
아울러 과장이 아니라 왜곡이나 틀린 부분이 있으면 언제든지 알려주시면, 수정하도록 하겠습니다.

## AI 관련 기본 개념

해당 글을 ratsgo's blog의 글을 참조하였으며, 굉장히 깔끔하게 개념들을 정리하기에 좋습니다.

관련 내용은 다음과 같습니다.

 - Entropy
 - Shannon Entropy
 - KL divergence
 - Cross Entropy
 
 ### Entorpy
 
엔트로피는 보통 열역학에서 처음 접했을 개념입니다. 
최근에는 AI 관련하여서 정보학/AI 에서 정보의 양이나 확률을 표현하는 방법으로 빈번하게 사용되고 있습니다.

정보이론에서는 정보량과 빈도에 따라서 효율적인 전송을 하기 위하여 고려되었던 사항입니다.
조금 더 구체적으로는 자주 발생하는 정보에 대해서는 더 적은 비트로 표현하는 것으로 처음 시작되었습니다.
  ex> (예를 들면 사막에 날씨를 표현) 
      맑음=00(빈도95%), 흐림=01(빈도4%), 비=10(빈도0.95%), 눈=11(빈도0.05%) 로 표현하는 것보다는,
      맑음=0(빈도95%), 흐림=1(빈도4%), 비=10(빈도0.95%), 눈=11(빈도0.05%) 로 표현하는 것이 통신을 더 효율적(적은 bit)로 할 수 있겠죠.

이제 정보량 측면을 보겠습니다. 사막에서 '맑음' 과 '눈'은 모두 날씨를 표현하지만, '맑은'과 '눈'은 전혀 상이한 정보량을 가지고 있습니다.
즉, 더 빈도가 낮은 정보가 더 많은 정보를 가지고 있습니다. 즉, '눈'이 더 많은 정보량을 가집니다.

유사한 예시를 하나 더 들자면,
  ex> (학생임을 가정) 학교에 간다, 공항에 간다.
공항에 가는 경우가 더욱 빈도가 낮으므로 특별한 이벤트이며, 더 정보량이 많다고 볼 수 있습니다.

위에 정보이론에서의 정보의 빈도에 대한 부분을 확률로 변경하여 생각하면 바로 AI에서 사용하는 엔트로피의 개념이 됩니다.

### Shannon Entropy

발생가능한 사건이나 메세지의 확률분포에 음의 로그를 취한 수식입니다. 

관련된 정보량 I(x)는 아래와 같습니다.
I(x)\quad =\quad -logP(x)
샤넌 엔트로피에 대한 수식은 아래와 같습니다.
H(P)\quad =\quad H(x)\quad =\quad -\sum { P(x)logP(x) } 

사건의 확률 분포가 균등적(Uniform)할 수록 엔트로피는 높아집니다. (eg> 공평한 주사위, 공평한 동전)
(위에 사막 날씨를 생각해보면 될 것 같습니다.)
공평한 동전을 던지는 경우 확률은 1/2, 샤넌 엔트로피는 1입니다.

### KL divergence

KL divergence(Kullbak-Leibler divergence)는 두 확률분포의 차이를 계산하는 데 사용하는 함수입니다.
추정치와 참값을 비교하는 지표로 사용합니다.

### Cross Entropy

KL divergence와 유사하게 추정치와 참값을 비교하는 지표로 많이 사용되며,
주로 onehot encoding을 적용한 classification의 loss function에서 주로 활용됩니다. 
